{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the k8s lab in the attic","text":"<p>This page contains public documentation, howto's and gotcha's about kubernetes and Tanzu.</p>"},{"location":"homelab/","title":"My homelab setup","text":"<p>Nothing beats something to call your own, independent of clouds, vpn's or corporate logins and policies</p>"},{"location":"homelab/#the-hardware","title":"The hardware","text":""},{"location":"homelab/#vsphere-lab-servers","title":"vSphere lab servers","text":"Comp Whitebox 1 Dell SFF 1 Dell SFF 2 CPU Intel Core i5 12500 Intel Core i3-10100 Intel Core i3-10100 Motherboard ASUS PRO B660M-C-D4-CSM Dell Optiplex Dell Optiplex Memory 2x Crucial Pro 64GB 3200MT 2x 32GB DDR4 3200MT 2x 32GB DDR4 3200MT OS disk Crucial MX100 256GB Western Digital Blue SN580 500 GB Western Digital Blue SN580 500 GB Datastore NVMe disk Samsung 980 PRO 2TB SK hynix Platinum P41 1TB SK hynix Platinum P41 1TB NVMe tiering disk Western Digital RED SN700 500GB NIC Intel X520-DA2 Intel i226 Intel i226 GPU Nvidia P4 n/a n/a PSU be quiet! SFX POWER 3 300W Dell Power brick Dell Power brick Enclosure Norco 2U short rackmount Dell Optiplex 7080 micro Dell Optiplex 7080 micro"},{"location":"homelab/#proxmox-lab-server","title":"Proxmox lab server","text":"Comp Hardware CPU Intel Core i3 9100 Motherboard Fujitsu D3643-H1 Memory 2x Crucial 32GB 3200MT kit OS disk Crucial MX100 256GB Datastore disk Western Digital Blue 1TB SSD NIC Intel X520-DA2 PSU PicoPSU 150-XT with Leicke ULL 120W Adapter Enclosure Norco 2U short rackmount"},{"location":"homelab/#nas","title":"NAS","text":"Comp Hardware CPU Intel Core i3 9100 Motherboard Fujitsu D3643-H1 Memory Crucial Ballistix 8GB DDR4 2400MT OS disk Western Digital Blue 1TB Data disk 1 Samsung 980 1TB NVMe Data disk 2 Toshiba MG08 16TB NIC Intel X520-DA2 PSU PicoPSU 150-XT with Leicke ULL 120W Adapter Enclosure Norco 2U 8bay rackmount"},{"location":"homelab/#software","title":"Software","text":"<p>My vSphere lab is currently set up as a three host vSphere cluster with the following software packages/versions.</p> Software Version vSphere 8U3 vCenter 8U3p1 NSX 4.2.0.1 AVI Loadbalancer 22.1.7 <p>My primary lab server is running Proxmox 8.4. I made the switch to proxmox a few months ago in an effort to reduce power usage to a minimum since this system is running 24/7. vSphere is not a power efficient Hypervisor since it doesn't really have to. </p> <p>At the time of the switch, the Broadcom aquisition was in full transition and internal vSphere licenses were no longer available. Not willing to drop money on VMUG just yet, I decided to go with Proxmox.</p>"},{"location":"homelab/#networking","title":"Networking","text":"<p>All lab servers are connected with a single 1 GbE connection for management. The vSphere whitebox has one 10GB SFP+ DAC connection for data while the Proxmox build and Dell SFF's use a 2.5GbE link.</p>"},{"location":"homelab/topology/","title":"Topology","text":"<p>Coming soooooon!</p>"},{"location":"k8s/","title":"Kubernetes","text":"<p>This section of the documentation handles everything kubernetes that is not related to the VMware kubernetes distribution.</p>"},{"location":"k8s/avi_with_k8s/","title":"Running AVI as a Layer 7 loadbalancer on vanilla kubernetes","text":"<p>Most of you will know about the integration of AVI Loadbalancer with NSX and VMware's WCP/TKG. In this page we will be deploying the AVI operator (AKO) on a kubeadm cluster to handle Layer 7 traffic.</p>"},{"location":"k8s/avi_with_k8s/#the-setup","title":"The setup","text":"<p>The high level topology looks like this: </p> <p>For the kubernetes part, we're running a 6 node kubeadm cluster (3 CP's, 3 workers) on Debian VM's on top of our Proxmox host. This cluster (dkv-kubeadm-dc) is running kubernetes version 1.30.1.  </p> <p>On the AVI side of things, we are running our AVI controller on the vSphere host. The Service Engines (SE's) will be running on the Proxmox host next to the k8s cluster. We will not cover the deployment of the AVI controller itself in this guide but any configuration required to get this working will be shown.</p> <p>Everything communicates across a flat L2 10.0.0.0/24 network.</p>"},{"location":"k8s/avi_with_k8s/#configuring-the-avi-cloud","title":"Configuring the AVI Cloud","text":"<p>Firstly we'll configure our AVI Cloud in the AVI Controller. </p>"},{"location":"k8s/avi_with_k8s/#cloud","title":"Cloud","text":"<p>For this setup, we will use the Default Cloud that already comes with a newly deployed AVI controller. If you already use this cloud, you can simply create a new one. The cloud type is going to be No Orchestrator. </p> <p>We will come back to this cloud once we've configured the IPAM and network.</p>"},{"location":"k8s/avi_with_k8s/#network","title":"Network","text":"<p>We will need to define a network form which the SE's will pick IP's for loadbalancing.</p> <p>Under Infrastructure &gt; Cloud Resources &gt; Networks, create a new network, give it a name and specify the IP range. Keep \"Use Static IP Address for VIPs and SE\" enabled and add the IP range.</p> <p></p>"},{"location":"k8s/avi_with_k8s/#ipam","title":"IPAM","text":"<p>To complete our Cloud configuration, we need to create a new IPAM profile. Create a new template under Templates &gt; Profiles &gt; IPAM/DNS Profiles. Give it a name, select your cloud and add the Network you created in the previous step.</p>"},{"location":"k8s/avi_with_k8s/#finish-cloud-config","title":"Finish Cloud config","text":"<p>Go back and edit your Cloud. In the IPAM section, select your newly created IPAM profile. </p>"},{"location":"k8s/avi_with_k8s/#deploying-the-ses","title":"Deploying the SE's","text":"<p>The previous steps were all pretty easy. Now comes the hard part: deploying Service Engines onto Proxmox. To deploy the SE's, I have taken the official AVI ansible role and edited it to be used with Proxmox.</p>"},{"location":"k8s/avi_with_k8s/#deploying-ako-on-the-kubeadm-cluster","title":"Deploying AKO on the kubeadm cluster","text":""},{"location":"k8s/custom_hpa/","title":"Scaling applications with HorizontalPodAutoscaling and custom metrics","text":""},{"location":"k8s/custom_hpa/#goal","title":"Goal","text":"<p>Being able to scale an application based on specific non cpu/memory metrics provided by the application.</p>"},{"location":"k8s/custom_hpa/#requirements","title":"Requirements","text":"<ul> <li>A running k8s cluster (duh).</li> <li>Prometheus operator running in the same cluster.</li> <li>Prometheus-adapter deployed. See addendum for deployment steps.</li> <li>Contour Ingress Controller (if you want to use the HTTPProxy in the example, otherwise swap it out for a normal ingress on a controller of your choice.)</li> <li>An application that exposes application-specific metrics. In our example we will use a simple NGINX deployment with nginx-prometheus-exporter but any application that exposes prometheus compatible metrics will do.</li> </ul>"},{"location":"k8s/custom_hpa/#procedure","title":"Procedure","text":""},{"location":"k8s/custom_hpa/#demo-application-nginx","title":"Demo application: Nginx","text":"<p>First, we will need to use an application that exposes custom metrics. In this example, we'll use a basic nginx webapp and expose the nginx stats. The nginx deployment uses the <code>nginx-prometheus-exporter</code> sidecar to expose nginx metrics in Prometheus format that can easily be scraped by Prometheus.</p> <ol> <li>Deploy the nginx yaml example file. <code>kubectl apply -f nginx.yaml</code></li> <li> <p>Once the nginx pods are up, you can validate the prometheus metrics by port-forwarding the service on the metrics port.  <code>kubectl port-forward svc/nginx 9113</code> </p> </li> <li> <p>Point your browser to <code>http://127.0.0.1:9113/metrics</code> to see the available metrics.</p> </li> <li>Deploy the ServiceMonitor yaml.     This instructs Prometheus to scrape the nginx service for metrics. <code>kubectl apply -f nginx-servicemonitor.yaml</code>     The ServiceMonitor references the nginx service which exposes metrics on port 9113.</li> <li>Verify that you can see the target marked UP in your Prometheus UI.     </li> <li> <p>Next up we'll verify that we can see our custom metrics for our pods. <code>kubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1\"/namespaces/nginx/</code> </p> <p>The output should look like this: </p> </li> </ol> <pre><code>{\n\"kind\": \"MetricValueList\",\n\"apiVersion\": \"custom.metrics.k8s.io/v1beta1\",\n\"metadata\": {},\n\"items\": [\n    {\n    \"describedObject\": {\n        \"kind\": \"Pod\",\n        \"namespace\": \"nginx\",\n        \"name\": \"nginx-6c855c5c77-p7jk6\",\n        \"apiVersion\": \"/v1\"\n    },\n    \"metricName\": \"nginx_connections_active\",\n    \"timestamp\": \"2024-02-16T09:05:23Z\",\n    \"value\": \"1\",\n    \"selector\": null\n    },\n    {\n    \"describedObject\": {\n        \"kind\": \"Pod\",\n        \"namespace\": \"nginx\",\n        \"name\": \"nginx-6c855c5c77-z5jwq\",\n        \"apiVersion\": \"/v1\"\n    },\n    \"metricName\": \"nginx_connections_active\",\n    \"timestamp\": \"2024-02-16T09:05:23Z\",\n    \"value\": \"1\",\n    \"selector\": null\n    }\n]\n}\n</code></pre>"},{"location":"k8s/custom_hpa/#configuring-horizontalpodautoscaling","title":"Configuring HorizontalPodAutoscaling","text":"<p>Now that our application is up and running and we have verified that our metrics are visible, we can configure HPA to use these metrics. The API reference for the HPA object are here on the kubernetes.io page.</p> <ol> <li> <p>Deploy the example nginx-hpa.yaml manifest.  <code>kubectl apply -f nginx-hpa.yaml</code></p> </li> <li> <p>Verify that the HPA is deployed succesfull with <code>kubectl get hpa</code>     output:  </p> </li> </ol> <pre><code>NAME    REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\nnginx   Deployment/nginx   1/100     2         10        2          19h\n</code></pre> <ol> <li>Now the fun begins. Let's put some load on the nginx deployment. I use <code>plow</code> for this but any application that can stress test a webapp will do.     In my example, I will be continuously sending 200 concurrent connections for 10 minutes.</li> <li>After a few seconds, we can already see the connection count rise on our two pods according to Prometheus,     </li> <li>The HPA will then kick in and will scale the deployment to satisfy the target.  </li> </ol> <pre><code>NAME    REFERENCE          TARGETS       MINPODS   MAXPODS   REPLICAS   AGE\nnginx   Deployment/nginx   105500m/100   2         10        8          19h\n</code></pre> <p>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0HPA will contiuously re-evaluate the metrics and scale accordingly  </p> <pre><code>NAME    REFERENCE          TARGETS      MINPODS   MAXPODS   REPLICAS   AGE\nnginx   Deployment/nginx   73666m/100   2         10        6          20h\n</code></pre> <p> See Addendum for an explanation on the numbers in the Targets column</p>"},{"location":"k8s/custom_hpa/#addendums","title":"Addendums","text":""},{"location":"k8s/custom_hpa/#installing-prometheus-adapter","title":"Installing Prometheus-adapter","text":"<p>If you have not deployed the Prometheus adapter yet, deploy it through the helm chart: 1. Add the helm repo. <code>helm repo add prometheus-community  https://prometheus-community.github.io/helm-charts</code> 2. Deploy the prometheus-adapter chart. <code>helm install prometheus-adapter prometheus-community/prometheus-adapter --set prometheus.url=\"http://prometheus-kube-prometheus-prometheus.prometheus.svc\",prometheus.port=\"9090\" --set rbac.create=\"true\" --namespace prometheus</code></p> <p>You will need prometheus-adapter to expose prometheus metrics under the custom.metrics.k8s.io API.</p>"},{"location":"k8s/custom_hpa/#list-available-custom-metrics","title":"List Available custom metrics","text":"<p>Once the prometheus-adapter is installed, query the custom metrics API. <code>kubectl get --raw \"/apis/custom.metrics.k8s.io/</code></p> <p>This will provide you with a full list of metrics you can use in HPA to scale against.</p>"},{"location":"k8s/custom_hpa/#understanding-the-current-and-target-metrics","title":"Understanding the current and target metrics","text":"<p>You might be puzzled by the output of the Targets column at first. In our example above, you can see that the current metric looks high (105500m/100) but this is due to the way kubernetes shows values in quantity. Quantity is explained here: https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/quantity/  </p> <p>In the above output the m means milli, meaning you need to divide the number by 1000. This results in a decimal value of 105,5 over the target of 100. </p> <p>Because the example HPA takes average values across the READY pods, the value might switch between being an integer and a float, which is why you will see the fixed-point representation of a number (105500m instead of 105,5) from time to time.</p>"},{"location":"tanzu/","title":"Anything Tanzu/TKG","text":""},{"location":"tanzu/#demystifying-the-tanzu-name","title":"Demystifying the \"Tanzu\" name","text":"<p>In it's current state, Broadcom has separated the kubernetes runtime from the Tanzu business unit and moved it under the VCF division. As a result, the Tanzu naming that is still broadly used, will disappear over time. </p> <p>This runtime is known under many names: - TKGs (Tanzu Kubernetes Grid Service) - WCP (Workload Control Plane) - Workload Management - vSphere with Tanzu - vSphere IAAS Control Plane - vSphere Supervisor  </p>"},{"location":"tanzu/argocd_wcp/","title":"Deploying VKS workload clusters with ArgoCD","text":"<p>With the addition of ArgoCD as a supervisor service, we can now easily deploy argoCD instances in vSphere namespaces and manage objects within these namespaces. One of the interesting things you can do is manage your workload cluster state with argoCD.  </p> <p>In this example we will be deploying workload clusters in a namespace on a supervisor and show how argoCD can reconcile your cluster state.</p>"},{"location":"tanzu/argocd_wcp/#prerequisites","title":"Prerequisites","text":"<ul> <li>A running supervisor cluster</li> <li>A vSphere namespace</li> <li>A git repo to store your cluster yamls</li> </ul>"},{"location":"tanzu/argocd_wcp/#deploying-argocd","title":"Deploying ArgoCD","text":""},{"location":"tanzu/argocd_wcp/#argocd-operator","title":"ArgoCD Operator","text":"<ul> <li>Download the operator yaml and configuration from the repo here</li> <li>Deploy the service through the \"Add new service\" wizard on the Services tab</li> <li>Enable the ArgoCD operator on your supervisor. This is done through \"Actions\" -&gt; \"Manage Service\"  in the Services tab.</li> <li>Once the operator has been enabled on the supervisor, you should be able to see the \"svc-argocd-operator-DOMAIN_X\" namespace and pods.</li> </ul>"},{"location":"tanzu/argocd_wcp/#deploy-an-argocd-instance-in-your-vsphere-namespace","title":"Deploy an ArgoCD instance in your vSphere namespace","text":"<p>With the ArgoCD operator installed, we can now deploy ArgoCD instances in our vSphere namespaces. You can follow the basic usage steps in this repo. Refer to the ArgoCD Operator documentation for more detailed options of your Argo instance. </p> <p>Attention: If you roll your own manifest, make sure to use the <code>kubernetes.io/os: CRX</code> nodeSelector. Otherwise the pods will never schedule on your ESXi hosts.</p> <ul> <li>Deploy your ArgoCD instance in your chosen namespace.</li> <li>Once the pods are created, point your browser towards the LoadBalancer IP your ArgoCD service received.</li> <li>The admin password can be found in the <code>MY_NAMESPACE-argocd-cluster</code> secret.</li> </ul>"},{"location":"tanzu/argocd_wcp/#creating-the-argo-application","title":"Creating the Argo application","text":"<p>You can either do this through the UI or CLI. Below is an example Application yaml:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  annotations:\n    argocd.argoproj.io/compare-options: ServerSideDiff=true\n  name: argocd-clusters\n  namespace: MY_NAMESPACE\nspec:\n  destination:\n    namespace: MY_NAMESPACE\n    server: https://kubernetes.default.svc\n  ignoreDifferences:\n  - group: cluster.x-k8s.io\n    jqPathExpressions:\n    - .spec.topology.variables[] | select(.name == \"TKR_DATA\")\n    - .spec.topology.variables[] | select(.name == \"user\")\n    - .spec.topology.variables[] | select(.name == \"clusterEncryptionConfigYaml\")\n    - .spec.topology.variables[] | select(.name == \"ntp\")\n    - .spec.topology.variables[] | select(.name == \"extensionCert\")\n    kind: Cluster\n  project: default\n  source:\n    path: .\n    repoURL: https://github.com/MY_REPO.git\n    targetRevision: HEAD\n</code></pre> <p>You can see the application already contains some additional fields to ignore diffs. More on that later.</p>"},{"location":"tanzu/argocd_wcp/#pushing-your-cluster-yaml-to-git-and-reconcile-with-argocd","title":"Pushing your cluster yaml to git and reconcile with ArgoCD.","text":""},{"location":"tanzu/nsx_for_wcp/","title":"Deploying NSX as part of a Workload Management deployment","text":"<p>When you want to leverage NSX networking for your vSphere Workload Management (called Supervisor hereafter) you need to obviously deploy NSX first. The following guide will go through the requirements and step-by-step deployment of NSX for use with WCP.</p>"},{"location":"tanzu/nsx_for_wcp/#architecture","title":"Architecture","text":""},{"location":"tanzu/rbac_on_wcp/","title":"Explaining RBAC on vSphere Supervisor and VKS guest clusters","text":"<p>vSphere Supervisor uses a concept of vSphere namespaces for tenancy which can be seen as a combination of vSphere resource pools and kubernetes namespaces.  Part of the featureset of a vSphere namespace is to assign permissions to users and groups. These users can be authenticated against either the vSphere SSO or an external OIDC provider.</p>"},{"location":"tanzu/rbac_on_wcp/#vsphere-namespace-permissions","title":"vSphere namespace permissions","text":"<p>The vSphere namespace permissions tile handles both Authentication and Authorization for a given namespace. All child objects within the namespace will inherit the given permission set. This means you need to carefully assign permissions if you want different permission sets within a namespace.</p>"},{"location":"tanzu/rbac_on_wcp/#kubernetes-rbac","title":"Kubernetes RBAC","text":"<p>vSphere SSO and external OIDC can provide authentication but you will probably still need granular authorization within a VKS guest cluster. This is where you will need to leverage the built-in kubernetes RBAC constructs with (cluster)rolebindings.</p>"},{"location":"tanzu/rbac_on_wcp/#deciding-what-permissions-you-need","title":"Deciding what permissions you need.","text":"<p>Depending on what authentication method you use and the required permissions on a guest cluster or namespace, you will need to set specific permissions both in the vSphere UI and through kubernetes RBAC.</p> <p>Below is a flowchart that guides you through the permissions and where to set them.</p> <p></p>"},{"location":"tanzu/wcp_issues/","title":"Supervisor issues","text":"<p>This page is meant as a high lvl KB for issues I've encountered with Supervisor.</p>"},{"location":"tanzu/wcp_issues/#cleanup","title":"Cleanup","text":""},{"location":"tanzu/wcp_issues/#unable-to-remove-nsx-bits-due-to-expired-nsx-password","title":"Unable to remove NSX bits due to expired NSX password","text":"<p>Symptoms: The supervisor removal is stuck on removing the NSX bits.   Logs:  </p> <pre><code>faultcode: ns0:FailedAuthentication\nfaultstring: Password of the user logging on is expired. :: Password of the user logging on is expired. :: User account expired: {Name: wcp-cluster-user-domain-REDACTED, Domain: vsphere.local}\nfaultxml: &lt;?xml version='1.0' encoding='UTF-8'?&gt;&lt;S:Envelope xmlns:S=\"http://schemas.xmlsoap.org/soap/envelope/\"&gt;&lt;S:Body&gt;&lt;S:Fault xmlns:ns4=\"http://www.w3.org/2003/05/soap-envelope\"&gt;&lt;faultcode xmlns:ns0=\"http://docs.oasis-open.org/ws-sx/ws-trust/200512\"&gt;ns0:FailedAuthentication&lt;/faultcode&gt;&lt;faultstring&gt;Password of the user logging on is expired. :: Password of the user logging on is expired. :: User account expired: {Name: wcp-cluster-user-domain-REDACTED, Domain: vsphere.local}&lt;/faultstring&gt;&lt;/S:Fault&gt;&lt;/S:Body&gt;&lt;/S:Envelope&gt;\n\n2024-10-01T02:09:28.793Z error wcp [kubelifecycle/cluster_network.go:233] [opID=6691222f-b2fc507c-d6ee-4e5d-a122-964730e97c76] Received error cleaning NCP-created resources for cluster domain-REDACTED on NSX Managers: REDACTED:443. Err: exit status 1\n</code></pre> <p>Root cause: When the password of the nsx user expires, the nsx cleanup script can no longer reach out to NSX to clean up the relevant NSX objects.</p> <p>Solution: 1. Edit the /usr/lib/vmware-wcp/nsx_policy_cleanup.py python script on the vCenter. 2. Set the following values:</p> <pre><code>self.vc endpoint = None\nself.vc_username = None\nself.vc_password = None \nself.username = \"admin\"\nself.password = \"My_admin_password\"\n</code></pre> <ol> <li>Restart the WCP service</li> </ol>"},{"location":"tanzu/wcp_oidc/","title":"End-to-end: Using Gitlab as an external OIDC for vSphere Supervisor","text":"<p>This guide is an end-to-end walkthrough for setting up Gitlab external OIDC access to VKS clusters. The guide has been written for vSphere 8 and has not been tested against VCF/VVF 9. The process will probably not change drastically but small differences may occur.</p>"},{"location":"tanzu/wcp_oidc/#why","title":"Why?","text":"<p>When you deploy workload clusters in your organization, you want to control who gets to do what on which cluster. RBAC is essential and vSpher eSupervisor offers two options out of the box:</p> <ol> <li>vSphere SSO - This leverages the vSphere SSO configuration.  </li> <li>External OIDC - This allows you to connect and external OIDC provider directly to the supervisor and VKS clusters.</li> </ol> <p>In this guide, we are going to use Gitlab as an external OIDC provider to authenticate users. Enabling an external OIDC provider also allows you to create a kubeconfig that can be shared among developers, since there are no credentials stored in the kubeconfig file. All authentication is done through pinniped.</p>"},{"location":"tanzu/wcp_oidc/#administrator-tasks","title":"Administrator tasks","text":""},{"location":"tanzu/wcp_oidc/#setting-up-gitlab","title":"Setting up Gitlab","text":"<ol> <li>In the Gitlab admin area, go to the application page and create a new application.</li> <li>Grab the redirect URL from the supervisor (Configure -&gt; Identity Providers in the vsphere Supervisor UI)</li> <li>Select Trusted and Confidential. Under scopes, select openid, profile and email.</li> <li>Copy the user id and the secret. The secret can only be read now!</li> </ol>"},{"location":"tanzu/wcp_oidc/#configuring-the-supervisor","title":"Configuring the supervisor","text":"<ol> <li>Back in the vSphere UI, create a new identity provider.</li> <li>fill in the following provider configuration:<ol> <li>Issuer URL: this is the Gitlab fqdn.</li> <li>Username claim: nickname</li> <li>Group claim: groups</li> </ol> </li> <li>Next, fill in the user id and secret.</li> <li>In additional settings, you must fill in openid in \"Additional Scopes\". Otherwise pinniped will automatically request the offline_access scope which does not exist in gitlab.</li> <li>If your gitlab instance is signed by a private CA, add the CA certificate.</li> </ol>"},{"location":"tanzu/wcp_oidc/#verifying-the-oidc-connection","title":"Verifying the OIDC connection","text":"<p>A siimple way of checking whether your Supervisor has succesfully connected to the OIDC provider is to get the OIDCIdentityProvider object in the vmware-system-pinniped namespace on the Supervisor.</p> <ol> <li>Log into the supervisor. Generally you'll do this through the vSphere SSO <code>kubectl vsphere login --server dkv-tanzu-mgmt.dklab.be  -u administrator@dklab.be</code></li> <li>Grab the active OIDCIdentityProvider and verify it's status is Ready </li> </ol> <pre><code>kubectl get OIDCIdentityProvider -n vmware-system-pinniped\nNAME       ISSUER                 STATUS   AGE\noidc-idp   https://git.dklab.be   Ready    20h\n\n</code></pre> <ol> <li>Describe the resulting object if you want more details   <code>kubectl describe OIDCIdentityProvider oidc-idp -n vmware-system-pinniped</code></li> </ol>"},{"location":"tanzu/wcp_oidc/#setting-up-the-tanzu-cli-and-generating-the-kubeconfig-file","title":"Setting up the Tanzu cli and generating the kubeconfig file","text":"<p>The next step is to generate a kubeconfig that leverages pinniped to connect to Gitlab.</p> <ol> <li>Check the correct version to download from this Compatibility matrix at the bottom of the page. For vSphere Supervisor and VKS, this is version 1.1.0.</li> <li>Once you've installed the tanzu binary, install the basic TKG group:</li> </ol> <pre><code>tanzu plugin install --group vmware-tkg/default\n</code></pre> <ol> <li>With the basic tanzu cli set up, connect to the supervisor cluster as an administrator. This will open a browser window to gitlab.</li> </ol> <pre><code>tanzu context create SVC_CLUSTER_NAME --endpoint https://SUPERVISOR_IP\n</code></pre> <ol> <li>Once you are logged in, check the cluster state</li> </ol> <pre><code>tanzu cluster list -n VSPHERE_NAMESPACE\n</code></pre> <ol> <li>Once you've verified your cluster is visible in the namespace, you can grab the user kubeconfig file</li> </ol> <pre><code>tanzu cluster kubeconfig get CLUSTER_NAME -n VSPHERE_NAMESPACE --export-file KUBECONFIG_FILE_PATH\n</code></pre> <p>Now you have exported a kubeconfig file that can be shared among developers. This kubeconfig file does not contain any user credentials but relies on the tanzu cli and the pinniped-auth plugin to connect to Gitlab and provide user authentication.</p>"},{"location":"tanzu/wcp_oidc/#setting-user-rights-on-the-cluster-andor-namespace","title":"Setting user rights on the cluster and/or namespace","text":"<p>The Authentication piece has been completed, but users still need the correct authorization on the vSphere namespace and/or workload cluster to do their job. See the flowchart and explanation here</p>"},{"location":"tanzu/wcp_oidc/#developeruser-tasks","title":"Developer/user tasks","text":"<p>As a developer, there are less steps to execute to be able to connect to the workload cluster.</p>"},{"location":"tanzu/wcp_oidc/#downloading-and-configuring-the-tanzu-cli","title":"Downloading and configuring the Tanzu cli","text":"<p>You can follow the same matrix as in the previous chapter to determine what Tanzu cli version you require. As mentioned above, for vSphere Supervisor on vSphere 8 this means version 1.1.0.</p> <ol> <li>Install the correct Tanzu cli version.</li> <li>Install the basic TKG group (this contains the required pinniped plugin)</li> </ol> <pre><code>tanzu plugin install --group vmware-tkg/default\n</code></pre>"},{"location":"tanzu/wcp_oidc/#using-the-generated-kubeconfig","title":"Using the generated kubeconfig","text":"<p>As a part of the Administrator workflow, you as a developer or user should have received a kubeconfig file from a platform administrator. You can now use the kubeconfig file as usual with kubectl. On first use, a browser window will open requiring you to log into Gitlab. Once logged in you can manage your resources according to what's allowed in the assigned RoleBinding or ClusterRoleBinding.</p>"}]}